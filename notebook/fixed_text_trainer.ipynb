{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82219d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/hoang/Downloads/data/output_numpy/passwords/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf01817",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"all_user_passwords.json\"\n",
    "with open(json_file, 'w') as f:\n",
    "    f.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a84253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20bcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 118/118 [00:50<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for user_dir in tqdm(sorted(os.listdir(data_dir))):\n",
    "    user_dir = os.path.join(data_dir, user_dir)\n",
    "    \n",
    "    data = {'genuine': [], 'impostor': [], 'text': '', 'user_id': ''}\n",
    "    \n",
    "    # genuine\n",
    "    genuine_dir = os.path.join(user_dir, 'genuine')\n",
    "    for timestamp_dir in os.listdir(genuine_dir):\n",
    "        timestamp_dir = os.path.join(genuine_dir, timestamp_dir)\n",
    "        if not os.path.isdir(timestamp_dir):\n",
    "            continue\n",
    "            \n",
    "        password_file = os.path.join(timestamp_dir, 'password.txt')\n",
    "        with open(password_file) as fpassword:\n",
    "            password_data = fpassword.read()\n",
    "        \n",
    "        press_file = os.path.join(timestamp_dir, 'p_raw_press.txt')\n",
    "        with open(press_file) as fpress:\n",
    "            press_data = [line.strip().split() for line in fpress]\n",
    "        release_file = os.path.join(timestamp_dir, 'p_raw_release.txt')\n",
    "        with open(release_file) as frelease:\n",
    "            release_data = [line.strip().split() for line in frelease]\n",
    "        \n",
    "        press_data = [{'type': 'down',\n",
    "                       'keycode': int(d[0]),\n",
    "                       'time': int(d[1])} for d in press_data]\n",
    "        release_data = [{'type': 'up',\n",
    "                       'keycode': int(d[0]),\n",
    "                       'time': int(d[1])} for d in release_data]\n",
    "        \n",
    "        data['genuine'].append(sorted(press_data + release_data, key=lambda x: x['time']))\n",
    "        \n",
    "    # impostor\n",
    "    impostor_dir = os.path.join(user_dir, 'impostor')\n",
    "    impostor_data = []\n",
    "    for timestamp_dir in os.listdir(impostor_dir):\n",
    "        timestamp_dir = os.path.join(impostor_dir, timestamp_dir)\n",
    "        if not os.path.isdir(timestamp_dir):\n",
    "            continue\n",
    "\n",
    "        password_file = os.path.join(timestamp_dir, 'password.txt')\n",
    "        with open(password_file) as fpassword:\n",
    "            password_data = fpassword.read()\n",
    "\n",
    "        press_file = os.path.join(timestamp_dir, 'p_raw_press.txt')\n",
    "        with open(press_file) as fpress:\n",
    "            press_data = [line.strip().split() for line in fpress]\n",
    "        release_file = os.path.join(timestamp_dir, 'p_raw_release.txt')\n",
    "        with open(release_file) as frelease:\n",
    "            release_data = [line.strip().split() for line in frelease]\n",
    "            \n",
    "        press_data = [{'type': 'down',\n",
    "                       'keycode': int(d[0]),\n",
    "                       'time': int(d[1])} for d in press_data]\n",
    "        release_data = [{'type': 'up',\n",
    "                       'keycode': int(d[0]),\n",
    "                       'time': int(d[1])} for d in release_data]\n",
    "        \n",
    "        data['impostor'].append(sorted(press_data + release_data, key=lambda x: x['time']))\n",
    "    \n",
    "    data['text'] = password_data\n",
    "    data['user_id'] = os.path.basename(user_dir)\n",
    "    \n",
    "    if len(data['genuine']) and len(data['impostor']):\n",
    "        with open(json_file, 'a') as fout:\n",
    "            fout.write(\"{}\\n\".format(json.dumps(data)))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9816781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Feature:\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_path=\"../configs/vocab.json\",\n",
    "            feature_type_path=\"../configs/feature_type.json\"\n",
    "    ):\n",
    "        with open(vocab_path) as f:\n",
    "            self.vocab = json.load(f)\n",
    "\n",
    "        with open(feature_type_path) as f:\n",
    "            self.feature_vocab = json.load(f)\n",
    "\n",
    "    def input_from_raw(self, raw_seq):\n",
    "        features = self.extract(raw_seq)\n",
    "\n",
    "        return self.input_from_feature(features)\n",
    "\n",
    "    def input_from_feature(self, features):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extract_key(self, sub_seq):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def agg_feature(self, feature, features=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def extract(self, raw_seq):\n",
    "        raw_seq = Feature.clean_raw_seq(raw_seq)\n",
    "        duration = (raw_seq[-1]['time'] - raw_seq[0]['time'])\n",
    "\n",
    "        features = None\n",
    "        for i in range(len(raw_seq)):\n",
    "            features = self.agg_feature(self.extract_key(raw_seq[i:]), features)\n",
    "\n",
    "        return features, duration\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_raw_seq(data):\n",
    "        data = sorted(data, key=lambda x: x[\"time\"])\n",
    "\n",
    "        i = 0\n",
    "        while i < len(data):\n",
    "            if \"keycode\" not in data[i]:\n",
    "                data.pop(i)\n",
    "                continue\n",
    "            i += 1\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class MatrixFeature(Feature):\n",
    "    def input_from_feature(self, features):\n",
    "        n_features = 5\n",
    "        n_keycodes = len(self.vocab)\n",
    "        feature_matrix = np.full((n_keycodes, n_keycodes, n_features), 0, dtype=np.float32)\n",
    "\n",
    "        for key, value in features.items():\n",
    "            key_items = key.split('_')\n",
    "            source_key = key_items[0]\n",
    "            feature_type = key_items[-1]\n",
    "            target_key = source_key if feature_type == \"Hold\" else key_items[1]\n",
    "\n",
    "            value = [item for item in value if (item < self.feature_vocab[feature_type][\"max\"]) and (item > 0)]\n",
    "\n",
    "            if not value:\n",
    "                continue\n",
    "\n",
    "            if source_key not in self.vocab:\n",
    "                continue\n",
    "\n",
    "            if target_key not in self.vocab:\n",
    "                continue\n",
    "\n",
    "            value = np.array(value)\n",
    "            feature_matrix[\n",
    "                self.vocab[source_key], self.vocab[target_key], self.feature_vocab[feature_type][\"index\"]\n",
    "            ] = np.mean(value)\n",
    "\n",
    "        feature_matrix = feature_matrix / 1000.\n",
    "\n",
    "        return feature_matrix\n",
    "\n",
    "    def extract_key(self, sub_seq):\n",
    "        features = dict()\n",
    "        source_down = {}\n",
    "        source_up = {}\n",
    "        target_down = {}\n",
    "        target_up = {}\n",
    "\n",
    "        for step_idx, step in enumerate(sub_seq):\n",
    "            if step[\"type\"] == \"down\":\n",
    "                if not source_down:\n",
    "                    source_down = step\n",
    "                    continue\n",
    "\n",
    "                if not target_down:\n",
    "                    target_down = step\n",
    "                    continue\n",
    "\n",
    "            if step[\"type\"] == \"up\":\n",
    "                if (not source_up) and source_down and (step[\"keycode\"] == source_down[\"keycode\"]):\n",
    "                    source_up = step\n",
    "                    continue\n",
    "\n",
    "                if (not target_up) and target_down and (step[\"keycode\"] == target_down[\"keycode\"]):\n",
    "                    target_up = step\n",
    "                    continue\n",
    "\n",
    "            if source_down and source_up and target_down and target_up:\n",
    "                break\n",
    "\n",
    "        if (not source_down) or (not source_up) or (not target_down) or (not target_up):\n",
    "            return {}\n",
    "\n",
    "        features[\"{}_{}_DD\".format(\n",
    "            source_down[\"keycode\"],\n",
    "            target_down[\"keycode\"]\n",
    "        )] = target_down[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_DU\".format(\n",
    "            source_down[\"keycode\"],\n",
    "            target_up[\"keycode\"]\n",
    "        )] = target_up[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_UD\".format(\n",
    "            source_up[\"keycode\"],\n",
    "            target_down[\"keycode\"]\n",
    "        )] = target_down[\"time\"] - source_up[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_UU\".format(\n",
    "            source_up[\"keycode\"],\n",
    "            target_up[\"keycode\"]\n",
    "        )] = target_up[\"time\"] - source_up[\"time\"]\n",
    "\n",
    "        features[\"{}_Hold\".format(\n",
    "            source_down[\"keycode\"]\n",
    "        )] = source_up[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def agg_feature(self, feature, features=None):\n",
    "        if not features:\n",
    "            features = dict()\n",
    "\n",
    "        for key, value in feature.items():\n",
    "            features[key] = features.get(key, [])\n",
    "            features[key].append(value)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class StatsFeature(Feature):\n",
    "    def input_from_feature(self, features):\n",
    "        n_features = 5\n",
    "        n_keycodes = len(self.vocab)\n",
    "        feature_mean = np.full((n_keycodes, n_features), 0, dtype=np.float32)\n",
    "        feature_std = np.full((n_keycodes, n_features), 0, dtype=np.float32)\n",
    "\n",
    "        for key, value in features.items():\n",
    "            key_items = key.split('_')\n",
    "            source_key = key_items[0]\n",
    "            feature_type = key_items[-1]\n",
    "\n",
    "            value = [item for item in value if (item < self.feature_vocab[feature_type][\"max\"]) and (item > 0)]\n",
    "\n",
    "            if not value:\n",
    "                continue\n",
    "\n",
    "            if source_key not in self.vocab:\n",
    "                continue\n",
    "\n",
    "            value = np.array(value)\n",
    "            feature_mean[self.vocab[source_key], self.feature_vocab[feature_type][\"index\"]] = np.mean(value)\n",
    "            feature_std[self.vocab[source_key], self.feature_vocab[feature_type][\"index\"]] = np.std(value)\n",
    "\n",
    "        feature_mean = feature_mean / 1000.\n",
    "        feature_std = feature_std / 1000.\n",
    "\n",
    "        return np.concatenate([feature_mean, feature_std], axis=-1)\n",
    "\n",
    "    def extract_key(self, sub_seq):\n",
    "        features = dict()\n",
    "        source_down = {}\n",
    "        source_up = {}\n",
    "        target_down = {}\n",
    "        target_up = {}\n",
    "\n",
    "        for step_idx, step in enumerate(sub_seq):\n",
    "            if step[\"type\"] == \"down\":\n",
    "                if not source_down:\n",
    "                    source_down = step\n",
    "                    continue\n",
    "\n",
    "                if not target_down:\n",
    "                    target_down = step\n",
    "                    continue\n",
    "\n",
    "            if step[\"type\"] == \"up\":\n",
    "                if (not source_up) and source_down and (step[\"keycode\"] == source_down[\"keycode\"]):\n",
    "                    source_up = step\n",
    "                    continue\n",
    "\n",
    "                if (not target_up) and target_down and (step[\"keycode\"] == target_down[\"keycode\"]):\n",
    "                    target_up = step\n",
    "                    continue\n",
    "\n",
    "            if source_down and source_up and target_down and target_up:\n",
    "                break\n",
    "\n",
    "        if (not source_down) or (not source_up) or (not target_down) or (not target_up):\n",
    "            return {}\n",
    "\n",
    "        features[\"{}_{}_DD\".format(\n",
    "            source_down[\"keycode\"],\n",
    "            target_down[\"keycode\"]\n",
    "        )] = target_down[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_DU\".format(\n",
    "            source_down[\"keycode\"],\n",
    "            target_up[\"keycode\"]\n",
    "        )] = target_up[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_UD\".format(\n",
    "            source_up[\"keycode\"],\n",
    "            target_down[\"keycode\"]\n",
    "        )] = target_down[\"time\"] - source_up[\"time\"]\n",
    "\n",
    "        features[\"{}_{}_UU\".format(\n",
    "            source_up[\"keycode\"],\n",
    "            target_up[\"keycode\"]\n",
    "        )] = target_up[\"time\"] - source_up[\"time\"]\n",
    "\n",
    "        features[\"{}_Hold\".format(\n",
    "            source_down[\"keycode\"]\n",
    "        )] = source_up[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def agg_feature(self, feature, features=None):\n",
    "        if not features:\n",
    "            features = dict()\n",
    "\n",
    "        for key, value in feature.items():\n",
    "            features[key] = features.get(key, [])\n",
    "            features[key].append(value)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class AnonymousSeqFeature(Feature):\n",
    "    def input_from_feature(self, features, duration, norm):\n",
    "        steps = []\n",
    "\n",
    "        for feature in features:\n",
    "            step = [None for _ in feature]\n",
    "            step[self.feature_vocab[\"DD\"][\"index\"]] = feature[\"DD\"]\n",
    "            step[self.feature_vocab[\"DU\"][\"index\"]] = feature[\"DU\"]\n",
    "            step[self.feature_vocab[\"UD\"][\"index\"]] = feature[\"UD\"]\n",
    "            step[self.feature_vocab[\"UU\"][\"index\"]] = feature[\"UU\"]\n",
    "            step[self.feature_vocab[\"Hold\"][\"index\"]] = feature[\"Hold\"]\n",
    "\n",
    "            steps.append(step)\n",
    "\n",
    "        steps = np.array(steps)\n",
    "        steps = np.clip(steps, -1500, 1500)\n",
    "#         if np.max(steps) > 1500:\n",
    "#             return\n",
    "\n",
    "        # normalize\n",
    "        # steps = steps * len(steps) / duration\n",
    "\n",
    "        if norm == 'max':\n",
    "            res = steps / np.max(steps)\n",
    "        elif norm == 'min_max':\n",
    "            res = (steps - np.min(steps)) / (np.max(steps) - np.min(steps))\n",
    "        elif norm == 'none':\n",
    "            res = steps / 1000.\n",
    "        else:\n",
    "            raise ValueError(\"Must norm\")\n",
    "\n",
    "        if np.isnan(res).any():\n",
    "            print(steps)\n",
    "        return res\n",
    "\n",
    "    def extract_key(self, sub_seq):\n",
    "        features = dict()\n",
    "        source_down = {}\n",
    "        source_up = {}\n",
    "        target_down = {}\n",
    "        target_up = {}\n",
    "\n",
    "        for step_idx, step in enumerate(sub_seq):\n",
    "            if step[\"type\"] == \"down\":\n",
    "                if not source_down:\n",
    "                    source_down = step\n",
    "                    continue\n",
    "\n",
    "                if not target_down:\n",
    "                    target_down = step\n",
    "                    continue\n",
    "\n",
    "            if step[\"type\"] == \"up\":\n",
    "                if step_idx == 0:\n",
    "                    return {}\n",
    "\n",
    "                if (not source_up) and source_down and (step[\"keycode\"] == source_down[\"keycode\"]):\n",
    "                    source_up = step\n",
    "                    continue\n",
    "\n",
    "                if (not target_up) and target_down and (step[\"keycode\"] == target_down[\"keycode\"]):\n",
    "                    target_up = step\n",
    "                    continue\n",
    "\n",
    "            if source_down and source_up and target_down and target_up:\n",
    "                break\n",
    "\n",
    "        if (not source_down) or (not source_up) or (not target_down) or (not target_up):\n",
    "            return {}\n",
    "\n",
    "        features[\"DD\"] = target_down[\"time\"] - source_down[\"time\"]\n",
    "        features[\"DU\"] = target_up[\"time\"] - source_down[\"time\"]\n",
    "        features[\"UD\"] = target_down[\"time\"] - source_up[\"time\"]\n",
    "        features[\"UU\"] = target_up[\"time\"] - source_up[\"time\"]\n",
    "        features[\"Hold\"] = source_up[\"time\"] - source_down[\"time\"]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def agg_feature(self, feature, features=None):\n",
    "        if not features:\n",
    "            features = list()\n",
    "\n",
    "        if feature:\n",
    "            features.append(feature)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c148ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AnonymousSeqFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "891409bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_tfrecord(data, label, filepath):\n",
    "    with tf.io.TFRecordWriter(filepath) as writer:\n",
    "        for i in range(len(data)):\n",
    "            features = tf.train.Features(\n",
    "                feature={\n",
    "                    \"data\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[data[i].astype(np.float32).tobytes()])),\n",
    "                    \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label[i]]))\n",
    "                }\n",
    "            )\n",
    "            example = tf.train.Example(features=features)\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_tfrecord(dirname):\n",
    "    filenames = [os.path.join(dirname, filename) for filename in sorted(os.listdir(dirname))]\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=4)\n",
    "    print(\"Load dataset contains {} records\".format(len(filenames)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7120788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_001 (14950, 2, 70, 5) (14950, 1)\n",
      "user_002 (14950, 2, 70, 5) (14950, 1)\n",
      "user_004 (14950, 2, 70, 5) (14950, 1)\n",
      "user_005 (13005, 2, 70, 5) (13005, 1)\n",
      "user_006 (14950, 2, 70, 5) (14950, 1)\n",
      "user_007 (4780, 2, 70, 5) (4780, 1)\n",
      "user_010 (219, 2, 70, 5) (219, 1)\n",
      "user_011 (6831, 2, 70, 5) (6831, 1)\n",
      "user_012 (1045, 2, 70, 5) (1045, 1)\n",
      "user_013 (14950, 2, 70, 5) (14950, 1)\n",
      "user_014 (721, 2, 70, 5) (721, 1)\n",
      "user_015 (1045, 2, 70, 5) (1045, 1)\n",
      "user_016 (6225, 2, 70, 5) (6225, 1)\n",
      "user_017 (3306, 2, 70, 5) (3306, 1)\n",
      "user_018 (100, 2, 70, 5) (100, 1)\n",
      "user_019 (14950, 2, 70, 5) (14950, 1)\n",
      "user_020 (14950, 2, 70, 5) (14950, 1)\n",
      "user_021 (49, 2, 70, 5) (49, 1)\n",
      "user_022 (13771, 2, 70, 5) (13771, 1)\n",
      "user_023 (14950, 2, 70, 5) (14950, 1)\n",
      "user_024 (14356, 2, 70, 5) (14356, 1)\n",
      "user_025 (3435, 2, 70, 5) (3435, 1)\n",
      "user_026 (14751, 2, 70, 5) (14751, 1)\n",
      "user_027 (6804, 2, 70, 5) (6804, 1)\n",
      "user_029 (2190, 2, 70, 5) (2190, 1)\n",
      "user_030 (14950, 2, 70, 5) (14950, 1)\n",
      "user_031 (12006, 2, 70, 5) (12006, 1)\n",
      "user_032 (7296, 2, 70, 5) (7296, 1)\n",
      "user_033 (10850, 2, 70, 5) (10850, 1)\n",
      "user_034 (928, 2, 70, 5) (928, 1)\n",
      "user_035 (14950, 2, 70, 5) (14950, 1)\n",
      "user_036 (1025, 2, 70, 5) (1025, 1)\n",
      "user_037 (14950, 2, 70, 5) (14950, 1)\n",
      "user_038 (1045, 2, 70, 5) (1045, 1)\n",
      "user_039 (5950, 2, 70, 5) (5950, 1)\n",
      "user_040 (7930, 2, 70, 5) (7930, 1)\n",
      "user_041 (11650, 2, 70, 5) (11650, 1)\n",
      "user_042 (14950, 2, 70, 5) (14950, 1)\n",
      "user_043 (1590, 2, 70, 5) (1590, 1)\n",
      "user_044 (10981, 2, 70, 5) (10981, 1)\n",
      "user_045 (6526, 2, 70, 5) (6526, 1)\n",
      "user_046 (11886, 2, 70, 5) (11886, 1)\n",
      "user_047 (14950, 2, 70, 5) (14950, 1)\n",
      "user_048 (7611, 2, 70, 5) (7611, 1)\n",
      "user_049 (14950, 2, 70, 5) (14950, 1)\n",
      "user_050 (13450, 2, 70, 5) (13450, 1)\n",
      "user_051 (1045, 2, 70, 5) (1045, 1)\n",
      "user_052 (14950, 2, 70, 5) (14950, 1)\n",
      "user_053 (721, 2, 70, 5) (721, 1)\n",
      "user_054 (14950, 2, 70, 5) (14950, 1)\n",
      "user_055 (6076, 2, 70, 5) (6076, 1)\n",
      "user_056 (9850, 2, 70, 5) (9850, 1)\n",
      "user_057 (1270, 2, 70, 5) (1270, 1)\n",
      "user_058 (828, 2, 70, 5) (828, 1)\n",
      "user_059 (6989, 2, 70, 5) (6989, 1)\n",
      "user_060 (4740, 2, 70, 5) (4740, 1)\n",
      "user_062 (14950, 2, 70, 5) (14950, 1)\n",
      "user_063 (1045, 2, 70, 5) (1045, 1)\n",
      "user_064 (2985, 2, 70, 5) (2985, 1)\n",
      "user_065 (11039, 2, 70, 5) (11039, 1)\n",
      "user_066 (1045, 2, 70, 5) (1045, 1)\n",
      "user_067 (3375, 2, 70, 5) (3375, 1)\n",
      "user_068 (6076, 2, 70, 5) (6076, 1)\n",
      "user_069 (3980, 2, 70, 5) (3980, 1)\n",
      "user_070 (1045, 2, 70, 5) (1045, 1)\n",
      "user_071 (1950, 2, 70, 5) (1950, 1)\n",
      "user_074 (7770, 2, 70, 5) (7770, 1)\n",
      "user_075 (11450, 2, 70, 5) (11450, 1)\n",
      "user_076 (660, 2, 70, 5) (660, 1)\n",
      "user_077 (8950, 2, 70, 5) (8950, 1)\n",
      "user_078 (10750, 2, 70, 5) (10750, 1)\n",
      "user_080 (995, 2, 70, 5) (995, 1)\n",
      "user_081 (335, 2, 70, 5) (335, 1)\n",
      "user_082 (162, 2, 70, 5) (162, 1)\n",
      "user_084 (7242, 2, 70, 5) (7242, 1)\n",
      "user_085 (3286, 2, 70, 5) (3286, 1)\n",
      "user_086 (1634, 2, 70, 5) (1634, 1)\n",
      "user_087 (9450, 2, 70, 5) (9450, 1)\n",
      "user_088 (485, 2, 70, 5) (485, 1)\n",
      "user_089 (8967, 2, 70, 5) (8967, 1)\n",
      "user_090 (245, 2, 70, 5) (245, 1)\n",
      "user_091 (195, 2, 70, 5) (195, 1)\n",
      "user_092 (1635, 2, 70, 5) (1635, 1)\n",
      "user_094 (590, 2, 70, 5) (590, 1)\n",
      "user_095 (585, 2, 70, 5) (585, 1)\n",
      "user_097 (9950, 2, 70, 5) (9950, 1)\n",
      "user_098 (430, 2, 70, 5) (430, 1)\n",
      "user_099 (295, 2, 70, 5) (295, 1)\n",
      "user_100 (990, 2, 70, 5) (990, 1)\n",
      "user_101 (667, 2, 70, 5) (667, 1)\n",
      "user_102 (735, 2, 70, 5) (735, 1)\n",
      "user_104 (885, 2, 70, 5) (885, 1)\n",
      "user_105 (1608, 2, 70, 5) (1608, 1)\n",
      "user_106 (345, 2, 70, 5) (345, 1)\n",
      "user_107 (1155, 2, 70, 5) (1155, 1)\n",
      "user_110 (126, 2, 70, 5) (126, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(json_file) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx > 100:\n",
    "            break\n",
    "        line = json.loads(line)\n",
    "        X = []\n",
    "        Y = []\n",
    "        text = line['text']\n",
    "        \n",
    "        for raw in line['genuine']:\n",
    "            if len(text)*2*(1-0.2) <= len(raw) <= len(text)*2*(1+0.2):\n",
    "                features, duration = feature_extractor.extract(raw)\n",
    "                x = feature_extractor.input_from_feature(features, duration, 'none')\n",
    "                if x is not None or not np.isnan(x).any():\n",
    "                    X.append(x)\n",
    "                    Y.append(1)\n",
    "                    \n",
    "        for raw in line['impostor']:\n",
    "            if len(text)*2*(1-0.2) <= len(raw) <= len(text)*2*(1+0.2):\n",
    "                features, duration = feature_extractor.extract(raw)\n",
    "                x = feature_extractor.input_from_feature(features, duration, 'none')\n",
    "                if x is not None or not np.isnan(x).any():\n",
    "                    X.append(x)\n",
    "                    Y.append(0)\n",
    "        \n",
    "        X = tf.keras.preprocessing.sequence.pad_sequences(X,\n",
    "                                                          padding=\"post\",\n",
    "                                                          truncating=\"post\",\n",
    "                                                          value=0,\n",
    "                                                          maxlen=70,\n",
    "                                                          dtype=\"float\")\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        X_pos = X[Y == 1]\n",
    "        X_neg = X[Y == 0]\n",
    "        X_pos = X_pos[:100]\n",
    "        X_neg = X_neg[:100]\n",
    "        if not len(X_pos) or not len(X_neg):\n",
    "            continue\n",
    "        \n",
    "        X_pairs = []\n",
    "        Y_pairs = []\n",
    "        for i in range(len(X_pos)):\n",
    "            anchor = X_pos[i]\n",
    "            positive_list = X_pos[i+1:]\n",
    "            for positive in positive_list:\n",
    "                X_pairs.append([anchor, positive])\n",
    "                Y_pairs.append([1])\n",
    "                \n",
    "        for anchor in X_pos:\n",
    "            for negative in X_neg:\n",
    "                X_pairs.append([anchor, negative])\n",
    "                Y_pairs.append([0])\n",
    "        \n",
    "        X_pairs = np.array(X_pairs)\n",
    "        Y_pairs = np.array(Y_pairs)\n",
    "        print(line[\"user_id\"], X_pairs.shape, Y_pairs.shape)\n",
    "        \n",
    "        save_tfrecord(X_pairs, Y_pairs, os.path.join(\"train\", line[\"user_id\"]+'.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc5bf98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_112 (4697, 2, 70, 5) (4697, 1)\n",
      "user_113 (3990, 2, 70, 5) (3990, 1)\n",
      "user_114 (4680, 2, 70, 5) (4680, 1)\n",
      "user_116 (5214, 2, 70, 5) (5214, 1)\n",
      "user_118 (7605, 2, 70, 5) (7605, 1)\n",
      "user_119 (395, 2, 70, 5) (395, 1)\n",
      "user_120 (175, 2, 70, 5) (175, 1)\n",
      "user_121 (46, 2, 70, 5) (46, 1)\n",
      "user_122 (445, 2, 70, 5) (445, 1)\n",
      "user_123 (1992, 2, 70, 5) (1992, 1)\n",
      "user_124 (95, 2, 70, 5) (95, 1)\n"
     ]
    }
   ],
   "source": [
    "with open(json_file) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx <= 100:\n",
    "            continue\n",
    "        line = json.loads(line)\n",
    "        X = []\n",
    "        Y = []\n",
    "        text = line['text']\n",
    "        \n",
    "        for raw in line['genuine']:\n",
    "            if len(text)*2*(1-0.2) <= len(raw) <= len(text)*2*(1+0.2):\n",
    "                features, duration = feature_extractor.extract(raw)\n",
    "                x = feature_extractor.input_from_feature(features, duration, 'none')\n",
    "                if x is not None or not np.isnan(x).any():\n",
    "                    X.append(x)\n",
    "                    Y.append(1)\n",
    "                    \n",
    "        for raw in line['impostor']:\n",
    "            if len(text)*2*(1-0.2) <= len(raw) <= len(text)*2*(1+0.2):\n",
    "                features, duration = feature_extractor.extract(raw)\n",
    "                x = feature_extractor.input_from_feature(features, duration, 'none')\n",
    "                if x is not None or not np.isnan(x).any():\n",
    "                    X.append(x)\n",
    "                    Y.append(0)\n",
    "        \n",
    "        X = tf.keras.preprocessing.sequence.pad_sequences(X,\n",
    "                                                          padding=\"post\",\n",
    "                                                          truncating=\"post\",\n",
    "                                                          value=0,\n",
    "                                                          maxlen=70,\n",
    "                                                          dtype=\"float\")\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        X_pos = X[Y == 1]\n",
    "        X_neg = X[Y == 0]\n",
    "        X_pos = X_pos[:100]\n",
    "        X_neg = X_neg[:100]\n",
    "        if not len(X_pos) or not len(X_neg):\n",
    "            continue\n",
    "        \n",
    "        X_pairs = []\n",
    "        Y_pairs = []\n",
    "        for i in range(len(X_pos)):\n",
    "            anchor = X_pos[i]\n",
    "            positive_list = X_pos[i+1:]\n",
    "            for positive in positive_list:\n",
    "                X_pairs.append([anchor, positive])\n",
    "                Y_pairs.append([1])\n",
    "                \n",
    "        for anchor in X_pos:\n",
    "            for negative in X_neg:\n",
    "                X_pairs.append([anchor, negative])\n",
    "                Y_pairs.append([0])\n",
    "        \n",
    "        X_pairs = np.array(X_pairs)\n",
    "        Y_pairs = np.array(Y_pairs)\n",
    "        print(line[\"user_id\"], X_pairs.shape, Y_pairs.shape)\n",
    "        \n",
    "        save_tfrecord(X_pairs, Y_pairs, os.path.join(\"dev\", line[\"user_id\"]+'.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3718fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
