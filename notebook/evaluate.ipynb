{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726c0972-9c11-4c91-b0ca-0285cdcdbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664da3a9-d522-404d-b921-1da6e3c8a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from scipy.spatial.distance import cdist\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c130d42-f535-4331-aabd-e80b9a76ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_2d(myList, v):\n",
    "    for i, x in enumerate(myList):\n",
    "        if v in x:\n",
    "            return (i, x.index(v))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376d2e87-b0b2-4547-ac09-5d710bd680dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(data):\n",
    "        \n",
    "    KEYS = [[\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"8\", \"8\"],\n",
    "            [\"\", \"81\", \"87\", \"69\", \"82\", \"84\", \"89\", \"85\", \"73\", \"79\", \"80\"],\n",
    "            [\"\", \"65\", \"83\", \"68\", \"70\", \"71\", \"72\", \"74\", \"75\", \"76\"],\n",
    "            [\"16\", \"90\", \"88\", \"67\", \"86\", \"66\", \"78\", \"77\", \"\", \"\", \"\", \"\"],\n",
    "            [\"\", \"\", \"\", \"\", \"32\", \"32\", \"32\", \"32\", \"32\"]]\n",
    "\n",
    "    KEYS_FLAT = [\"81\", \"87\", \"69\", \"82\", \"84\", \"89\", \"85\", \"73\", \"79\", \"80\", \"65\",\n",
    "                 \"83\", \"68\", \"70\", \"71\", \"72\", \"74\", \"75\", \"76\", \"90\", \"88\", \"67\",\n",
    "                 \"86\", \"66\", \"78\", \"77\", \"32\", \"16\", \"8\"]\n",
    "    \n",
    "    pressedKeys = []\n",
    "    for d in data:\n",
    "        pressedKeys.append((str(d['keycode']), d['press_time'], d['release_time']))\n",
    "\n",
    "    feature = []\n",
    "    min_time = 300\n",
    "    max_time = 1500\n",
    "    \n",
    "    for i in range(len(pressedKeys)):\n",
    "                \n",
    "        if i == len(pressedKeys) - 1:\n",
    "            break\n",
    "        \n",
    "        ht1 = int(pressedKeys[i][2]) - int(pressedKeys[i][1])\n",
    "        if ht1 > min_time:\n",
    "            ht1 = min_time\n",
    "\n",
    "        ht2 = int(pressedKeys[i + 1][2]) - int(pressedKeys[i + 1][1])\n",
    "        if ht2 > min_time:\n",
    "            ht2 = min_time\n",
    "\n",
    "        ptp = int(pressedKeys[i + 1][1]) - int(pressedKeys[i][1])\n",
    "        if ptp > max_time:\n",
    "            ptp = max_time\n",
    "\n",
    "        rtp = int(pressedKeys[i + 1][1]) - int(pressedKeys[i][2])\n",
    "        if rtp > max_time:\n",
    "            rtp = max_time\n",
    "\n",
    "        key1 = pressedKeys[i][0]\n",
    "        key2 = pressedKeys[i + 1][0]\n",
    "\n",
    "        d_key1 = index_2d(KEYS, key1)\n",
    "        d_key2 = index_2d(KEYS, key2)\n",
    "        \n",
    "        if not d_key1 or not d_key2:\n",
    "            continue\n",
    "            \n",
    "        keyDistance = np.sum(np.absolute(np.array(d_key1) - np.array(d_key2)))\n",
    "        # keyDistance = np.array(index_2d(KEYS, key1)) - np.array(index_2d(KEYS, key2))\n",
    "        feature.append((keyDistance / 15,\n",
    "                        ht1 / max_time,\n",
    "                        ht2 / max_time,\n",
    "                        ptp / max_time,\n",
    "                        rtp / max_time))\n",
    "                \n",
    "    # preprocessing\n",
    "    n_features = 5\n",
    "    maxlen = 100\n",
    "\n",
    "    feature = np.array(feature)[ : maxlen]\n",
    "    padding = np.full((maxlen, n_features), 0., dtype=np.float32)\n",
    "    padding[ : len(feature), :] = feature\n",
    "\n",
    "    return np.expand_dims(padding, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37df4b3-d419-4cbc-b186-368ca639fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, start_idx, end_idx):\n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if start_idx <= idx < end_idx:\n",
    "                line = json.loads(line)\n",
    "                data.append(line)\n",
    "            if idx >= end_idx:\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c1cffe-e100-4db7-80d6-65c83a60071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subsequence(sequences, maxlen=100, overlap=0):\n",
    "    \n",
    "    flatten = []\n",
    "    for seq in sequences:\n",
    "        flatten.extend(seq)\n",
    "    sequences = flatten\n",
    "    sequences = sorted(sequences, key=lambda x: x['press_time'])\n",
    "    \n",
    "    subsequence = []\n",
    "    for i in range(0, len(sequences), maxlen - overlap):\n",
    "        sub = sequences[i: i + maxlen]\n",
    "        if len(sub) >= 0.8 * maxlen:\n",
    "            subsequence.append(sub)\n",
    "    \n",
    "    return subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0383b6b0-9481-451f-9a36-642c8248a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [(0, 500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (500, 1000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (1000, 1500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (1500, 2000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (2000, 2500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (2500, 3000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (3000, 3500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (3500, 4000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (4000, 4500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (4500, 5000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (5000, 5500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (5500, 6000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (6000, 6500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (6500, 7000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (7000, 7500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (7500, 8000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (8000, 8500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (8500, 9000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (9000, 9500, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving'), (9500, 10000, '/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving')]\n"
     ]
    }
   ],
   "source": [
    "num_users = 10000\n",
    "batch_users = 500\n",
    "\n",
    "args = [(i, i + batch_users, \"/home/anhtt_vcs/Public/hoangp46/typing-net/ckpt/base/serving\") for i in range(0, num_users, batch_users)]\n",
    "print(len(args), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ced575fc-f8d2-4308-8069-a99d53ddfdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "    \n",
    "    start_idx, end_idx, model_path = params\n",
    "        \n",
    "    metric = 'cosine'\n",
    "    threshold = 0.5\n",
    "    negative_sample = 100\n",
    "    maxlen = 100\n",
    "    overlap = 0\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    data = read_data(file_path=\"/home/anhtt_vcs/Public/keystrokes_feature/all_by_user.json\",\n",
    "                     start_idx=start_idx,\n",
    "                     end_idx=end_idx)\n",
    "\n",
    "    for i, user_data in tqdm(enumerate(data)):\n",
    "\n",
    "        user_id = user_data['label']\n",
    "        sequences = user_data['sequences']\n",
    "        sequences = split_subsequence(sequences, maxlen)\n",
    "\n",
    "        positive = []\n",
    "        for sequence in sequences:\n",
    "            x = parser(sequence)\n",
    "            positive.append(x)\n",
    "        positive = np.concatenate(positive)\n",
    "\n",
    "        negative = []\n",
    "        negative_user = data[:i] + data[i + 1:] # remove user i-th\n",
    "        negative_user = random.sample(negative_user, negative_sample)\n",
    "        for negative_data in negative_user:\n",
    "            negative_id = negative_data['label']\n",
    "            negative_sequences = negative_data['sequences']\n",
    "            negative_sequences = split_subsequence(negative_sequences)\n",
    "            for sequence in negative_sequences:\n",
    "                x = parser(sequence)\n",
    "                negative.append(x)\n",
    "        negative = np.concatenate(negative)\n",
    "\n",
    "        # get embedding\n",
    "        positive = model(positive)\n",
    "        negative = model(negative)\n",
    "\n",
    "        positive_dist = cdist(positive, positive, metric).round(4)\n",
    "        negative_dist = cdist(positive, negative, metric).round(4)\n",
    "\n",
    "        positive_dist = np.reshape(positive_dist, (-1,))\n",
    "        positive_dist = positive_dist[positive_dist >= 1e-6]\n",
    "        negative_dist = np.reshape(negative_dist, (-1,))\n",
    "\n",
    "        fn = np.sum(negative_dist < threshold) / len(negative_dist) \n",
    "        fp = np.sum(positive_dist > threshold) / len(positive_dist)\n",
    "\n",
    "        log = {\"id\": user_id,\n",
    "               \"fp\": float(np.mean(fp) * 100),\n",
    "               \"fn\": float(np.mean(fn) * 100)}\n",
    "        \n",
    "        dist_log = {\"id\": user_id,\n",
    "                    \"d_positive\": positive_dist.tolist(),\n",
    "                    \"d_negative\": negative_dist.tolist()}\n",
    "\n",
    "        with open(\"eval_{}_threshold_{}_negatives_{}_maxlen_{}.json\".format(metric, threshold, negative_sample, maxlen), \"a\") as f:\n",
    "            f.write(json.dumps(log)+\"\\n\")\n",
    "            \n",
    "        with open(\"eval_{}_distance_negatives_{}_maxlen_{}.json\".format(metric, negative_sample, maxlen), \"a\") as f:\n",
    "            f.write(json.dumps(dist_log)+\"\\n\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5393a15d-57c8-4bf1-ae21-4ff672252d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:43,  2.07s/it]\n",
      "44it [01:30,  2.05s/it]\n",
      "139it [04:44,  2.05s/it]\n",
      "277it [09:05,  1.97s/it]\n",
      "334it [11:02,  1.85s/it]/home/anhtt_vcs/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in long_scalars\n",
      "377it [12:13,  1.82s/it]\n",
      "500it [15:40,  1.88s/it]\n",
      "500it [15:43,  1.89s/it]\n",
      "500it [15:44,  1.89s/it]\n",
      "500it [15:45,  1.89s/it]\n",
      "500it [15:45,  1.89s/it]\n",
      "500it [15:45,  1.89s/it]\n",
      "500it [15:47,  1.89s/it]\n",
      "500it [15:48,  1.90s/it]\n",
      "500it [15:48,  1.90s/it]\n",
      "500it [15:48,  1.90s/it]\n",
      "500it [15:48,  1.90s/it]\n",
      "500it [15:50,  1.90s/it]\n",
      "500it [15:50,  1.90s/it]\n",
      "500it [15:50,  1.90s/it]\n",
      "500it [15:51,  1.90s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-8-5ea1d3aa68c0>\", line 32, in evaluate\n    positive = np.concatenate(positive)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: need at least one array to concatenate\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c598cb582a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         '''\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "pool = Pool(len(args))\n",
    "pool.map(evaluate, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8208b5f-c627-489a-9fbc-bf255e03abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"eval_cosine_threshold_0.5_negatives_100_maxlen_100.json\") as f:\n",
    "    df = [json.loads(line) for line in f]\n",
    "    \n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863edcdb-4d57-45d0-bd54-a0f7de927fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.51297121370114, 20.088107744210422)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fp'].mean(), df['fn'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f133e6f-3272-44db-85f7-1672258fa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"eval_cosine_threshold_0.5_negatives_100_maxlen_100.json\") as f:\n",
    "    df = [json.loads(line) for line in f]\n",
    "    \n",
    "df = pd.DataFrame(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
